Un computer (pronuncia italiana: /komˈpjuter/[1]), in italiano elaboratore o calcolatore (vedi «aspetti linguistici»), è una macchina automatizzata programmabile in grado di eseguire sia complessi calcoli matematici (calcolatore) sia altri tipi di elaborazioni dati (elaboratore).[2][3]

Concepito come una macchina per automatizzare alcune capacità della mente umana, come ad esempio il calcolo e la capacità di memorizzazione potenziandone la portata e applicandole alla soluzione di particolari problemi scientifici e ingegneristici[4], solo a partire dalla seconda metà del XX secolo, evolve in macchina in grado di eseguire le elaborazioni dati più varie. Ci si riferisce comunemente al computer come ad un dispositivo elettronico e digitale, programmabile a scopo generico, costruito secondo il modello teorico-computazionale della cosiddetta macchina di Turing e la cosiddetta architettura di von Neumann, anche se oggi tuttavia il termine assume il significato più generico di sistema elettronico di elaborazione programmabile e non programmabile, includendo dunque una vasta gamma di tipologie di dispositivi: sebbene i computer programmabili a scopo generico siano oggi i più diffusi, esistono infatti in specifici ambiti di applicazione modelli di computer dedicati a vari campi e settori come automazione industriale, domotica, computer grafica.

Nel corso della storia, l'implementazione tecnologica di questa macchina si è modificata profondamente sia nei meccanismi di funzionamento (meccanici, elettromeccanici ed elettronici), che nelle modalità di rappresentazione dell'informazione (analogica e digitale) che in altre caratteristiche (architettura interna, programmabilità, ecc.). In questa forma e al pari della televisione, esso rappresenta il mezzo tecnologico simbolo che ha maggiormente modificato le abitudini umane dopo la seconda guerra mondiale: la sua invenzione ha contribuito alla nascita e allo sviluppo dell'informatica moderna, che ha segnato l'avvento della cosiddetta terza rivoluzione industriale e della società dell'informazione.

Come per gran parte della terminologia informatica, tra il termine italiano «elaboratore» e il termine «computer» mutuato dall'inglese, nel linguaggio comune prevale nettamente l'uso del termine «computer».[5] In altre lingue europee accade diversamente: nella lingua francese si usa il termine ordinateur; nella lingua spagnola si usano i termini computadora e ordenador. La tendenza di usare parole inglesi è spesso biasimata in una diatriba sull'esterofilia della lingua italiana recente,[6][7] ma le proposte alternative, come il computiere[8] del professor Arrigo Castellani, accademico della Crusca e fondatore degli Studi Linguistici Italiani, non hanno ancora vasta applicazione. Negli anni 1960 e 1970 è stato utilizzato anche il termine «ordinatore», oggi in disuso, calco linguistico sul francese ordinateur.

In italiano il termine computer significa «calcolatore», che però ha un significato più ampio: può indicare anche una macchina non automatizzata (come ad esempio un regolo calcolatore), oppure una macchina automatizzata in grado di eseguire esclusivamente semplici calcoli matematici (come ad esempio una macchina addizionatrice). Nel secolo scorso poteva indicare anche un essere umano: la figura di «calcolatore» era un ruolo presso alcuni Osservatori astronomici italiani.

Il termine computer è il nome d'agente del verbo inglese to compute, derivato il francese computer.[9] L'etimo latino è composto da com = cum (insieme) e putare (tagliare, rendere netto – da cui l'odierno potare) e significa propriamente: «confrontare (o comparare) per trarre la somma netta».[10] In inglese, il termine indicava originariamente un essere umano,[11] incaricato di eseguire dei calcoli. Il primo utilizzo nel senso moderno è attestato nel 1947,[12][13] ma bisognerà attendere la metà degli anni 50 perché questa accezione diventi di uso comune (si notino, a questo proposito, i diversi acronimi dei computer ASCC ed ENIAC).

Il computer è la versione più evoluta di una serie di strumenti di calcolo inventati sin dall'antichità: l'abaco, la macchina di Anticitera, i bastoncini di Nepero. Gli esemplari di macchine calcolatrici più famosi sono forse la macchina di Pascal (1645) e la macchina di Leibniz (1672), ma va ricordata anche la macchina calcolatrice di Wilhelm Schickard, del 1623, della quale sono rimasti soltanto i progetti.

Il passaggio da macchina calcolatrice a vero e proprio computer (nel senso di dispositivo programmabile) si deve a Charles Babbage: la sua Macchina analitica, progettata nel 1833 ma mai realizzata, è il primo computer della storia. Si trattava di una colossale macchina a ingranaggi, alimentata a vapore e dotata di input, output, unità di memoria, di unità di calcolo decimale con registro dell'accumulo dei dati e di un sistema di collegamento tra le varie parti. Contrariamente a quanto si potrebbe pensare, la macchina analitica era interamente digitale.[14]

Nel corso dei decenni successivi il computer è passato attraverso vari stadi: il computer analogico (ne è un esempio l'analizzatore differenziale di Vannevar Bush del 1927), la macchina di Turing, i computer digitali meccanici ed elettromeccanici (la Serie Z di Konrad Zuse, la macchina di Stibitz e l'ASCC di Howard Aiken) ed infine quelli digitali ed elettronici (l'ABC di John Vincent Atanasoff e Clifford Berry, l'ENIAC di John Presper Eckert e John Mauchly, il Colossus britannico). Nel corso del XX secolo, inoltre, importanti progressi nel campo dell'elettronica - come il transistor e il circuito integrato - e dell'informatica hanno contribuito all'evoluzione del computer nella sua forma attuale passando da dispositivo elettronico presente solo in aziende e centri di ricerca a dispositivo ad uso comune e consumo di massa per gli utenti comuni.

Descrizione